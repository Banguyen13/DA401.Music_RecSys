{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "from yt_dlp import YoutubeDL\n",
    "import subprocess\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppose your MPD is saved in a folder name MPD\n",
    "mil_playlists=os.listdir(\"MPD\")\n",
    "all_songs=[]\n",
    "#Optional: Count the number of songs with duplicates to see how many unique songs there are in the MPD\n",
    "def song_count(files, sample_size=100000):\n",
    "    total = 0  # Total count of songs\n",
    "    for file in files:\n",
    "        with open(f\"MSD/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            for playlist in data.get(\"playlists\", []):\n",
    "                for track in playlist.get(\"tracks\", []):\n",
    "                    total += 1\n",
    "    return total\n",
    "## Sample 100,000(or any number) unique songs from MPD\n",
    "def reservoir_sampling_unique(files, sample_size=100000):\n",
    "    reservoir = []          # List to hold sampled unique songs\n",
    "    unique_ids = set()      # Set to keep track of which songs have been added\n",
    "    unique_count = 0        # Count of unique songs seen so far\n",
    "\n",
    "    for file in files:\n",
    "        with open(f\"MSD/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            for playlist in data.get(\"playlists\", []):\n",
    "                for track in playlist.get(\"tracks\", []):\n",
    "                    song_id = track[\"track_uri\"]\n",
    "                    # If this song has already been added, skip it.\n",
    "                    if song_id in unique_ids:\n",
    "                        continue\n",
    "                    \n",
    "                    unique_count += 1  # New unique song encountered\n",
    "                    song = {\n",
    "                        \"title\": track[\"track_name\"],\n",
    "                        \"artist\": track[\"artist_name\"],\n",
    "                        \"SpotifyID\": song_id\n",
    "                    }\n",
    "                    \n",
    "                    if len(reservoir) < sample_size:\n",
    "                        # Reservoir not yet full, add the song and mark its ID as seen.\n",
    "                        reservoir.append(song)\n",
    "                        unique_ids.add(song_id)\n",
    "                    else:\n",
    "                        # Reservoir is full, decide whether to include this new unique song.\n",
    "                        j = random.randint(0, unique_count - 1)\n",
    "                        if j < sample_size:\n",
    "                            # Replace the song at index j.\n",
    "                            removed_song = reservoir[j]\n",
    "                            unique_ids.remove(removed_song[\"SpotifyID\"])\n",
    "                            reservoir[j] = song\n",
    "                            unique_ids.add(song_id)\n",
    "    return reservoir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 100,000 songs and save them to a csv\n",
    "sampled_songs = reservoir_sampling_unique(mil_playlists, sample_size=100000)\n",
    "sampled_df=pd.DataFrame(sampled_songs)\n",
    "sampled_df.to_csv(\"sampled_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Audio processing with librosa and Lyrics embeddings extractions with lyricsgenius and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio(song_title, artist, output_filename=\"downloaded_audio\", max_duration=600, max_filesize=20*1024*1024):\n",
    "    \"\"\"\n",
    "    Downloads audio from YouTube using yt_dlp Python API.\n",
    "    Filters for songs under max_duration seconds and filesize (here's 10 minutes and 20 Mb) in case some songs are not found on YouTube and mistaken for big files that slow the whole process down\n",
    "    Store the audio in a temporary file, process it through librosa to extract embeddings before clearing the files\n",
    "    Returns filename if successful; otherwise, None.\n",
    "    \"\"\"\n",
    "    # Ensure the output template includes the extension placeholder.\n",
    "    ## Depending on your ffmpeg, you might need to reencode your audio file differently. I have two version of reencoding but this version runs well in Jupiter environment\n",
    "    ## Request for another version if your code doesn't run properly\n",
    "    if \"%(ext)s\" not in output_filename:\n",
    "        output_filename = output_filename + \".%(ext)s\"\n",
    "        \n",
    "    query = f\"{song_title} {artist} official audio\"\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': output_filename,\n",
    "        'noplaylist': True,\n",
    "        'quiet': True,\n",
    "        'no_warnings': True,\n",
    "        'default_search': 'ytsearch',\n",
    "        'match_filter': lambda info_dict: None if (info_dict.get('duration', 0) < max_duration and info_dict.get('filesize_approx', 0) < max_filesize) else 'Video too long or file too large',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'prefer_ffmpeg': True\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            info_dict = ydl.extract_info(query, download=True)\n",
    "            # Use prepare_filename to get the downloaded file, then change its extension to .mp3\n",
    "            base_filename = os.path.splitext(ydl.prepare_filename(info_dict))[0]\n",
    "            downloaded_file = base_filename + \".mp3\"\n",
    "            if os.path.exists(downloaded_file):\n",
    "                return downloaded_file\n",
    "            else:\n",
    "                print(\"Downloaded file not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download error: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_audio_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Extract audio features from the MP3 file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        rms_mean = np.mean(librosa.feature.rms(y=y))\n",
    "        spectral_centroid_mean = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        mfccs_mean = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)\n",
    "        chroma_cens_mean = np.mean(librosa.feature.chroma_cens(y=y, sr=sr), axis=1)\n",
    "        tonnetz_mean = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr), axis=1)\n",
    "\n",
    "        embeddings = {\n",
    "            'tempo': tempo,\n",
    "            'rms_mean': rms_mean,\n",
    "            'spectral_centroid_mean': spectral_centroid_mean,\n",
    "            'mfccs_mean': mfccs_mean,\n",
    "            'chroma_cens_mean': chroma_cens_mean,\n",
    "            'tonnetz_mean': tonnetz_mean\n",
    "        }\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_audio_embeddings(song_title, artist):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Download audio and extract embeddings.\n",
    "    Create a temporary file, extract embeddings then clean up the files. \n",
    "    \"\"\"\n",
    "    downloaded_file = download_audio(song_title, artist)\n",
    "    if not downloaded_file:\n",
    "        print(f\"Failed to download '{song_title}' by '{artist}'.\")\n",
    "        return None\n",
    "\n",
    "    embeddings = extract_audio_embeddings(downloaded_file)\n",
    "\n",
    "    # Clean up downloaded audio\n",
    "    if os.path.exists(downloaded_file):\n",
    "        os.remove(downloaded_file)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: Flatten out the audio embeddings to separate different features into their own columns\n",
    "def flatten_features(features_dict):\n",
    "    flat = {}\n",
    "    for key, value in features_dict.items():\n",
    "        # Check if the value is a list or a NumPy array\n",
    "        if isinstance(value, (list, np.ndarray)):\n",
    "            arr = np.array(value)\n",
    "            # If it's a 0-dim array or a single element array, assign the scalar value\n",
    "            if arr.ndim == 0 or (arr.ndim == 1 and arr.size == 1):\n",
    "                flat[key] = arr.item()\n",
    "            elif arr.ndim == 1:\n",
    "                for i, v in enumerate(arr, start=1):\n",
    "                    flat[f\"{key}_{i}\"] = v\n",
    "            else:\n",
    "                # For multi-dimensional arrays, flatten completely and add indices (optional)\n",
    "                flat_array = arr.flatten()\n",
    "                for i, v in enumerate(flat_array, start=1):\n",
    "                    flat[f\"{key}_{i}\"] = v\n",
    "        else:\n",
    "            flat[key] = value\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def process_dataset(df, batch_size=1000, output_dir=\"output_batches\", sleep_time=1):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame with columns 'title' and 'artist' in batches.\n",
    "    \n",
    "    For each song in a batch:\n",
    "      - Calls get_audio_embeddings(title, artist)\n",
    "      - Flattens the returned embeddings using flatten_features\n",
    "      - Adds a \"failed\" flag (True if the song couldn’t be processed)\n",
    "    \n",
    "    Each batch is saved as a separate Parquet file in output_dir.\n",
    "    This way, even if the kernel dies, you'll have saved batches of progress.\n",
    "    \n",
    "    Parameters:\n",
    "      - df: DataFrame with columns \"title\" and \"artist\".\n",
    "      - batch_size: Number of songs to process per batch.\n",
    "      - output_dir: Directory where batch Parquet files will be saved.\n",
    "      - sleep_time: Pause (in seconds) between processing songs.\n",
    "      \n",
    "    Returns:\n",
    "      - A folder with parquet files, each containing 1000(number per batch) processed songs with their embeddings\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    total = len(df)\n",
    "    batch_files = []\n",
    "    \n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        print(f\"Processing batch from index {start} to {end} out of {total}\")\n",
    "        batch_results = []\n",
    "        \n",
    "        for idx, row in df.iloc[start:end].iterrows():\n",
    "            title = row[\"title\"]\n",
    "            artist = row[\"artist\"]\n",
    "            print(f\"Processing song {idx}: '{title}' by '{artist}'...\")\n",
    "            \n",
    "            # Process the song and get embeddings.\n",
    "            embeddings = get_audio_embeddings(title, artist)\n",
    "            \n",
    "            result = {\"original_index\": idx, \"title\": title, \"artist\": artist}\n",
    "            if embeddings is not None:\n",
    "                flat = flatten_features(embeddings)\n",
    "                result.update(flat)\n",
    "                result[\"failed\"] = False\n",
    "            else:\n",
    "                result[\"failed\"] = True\n",
    "            \n",
    "            batch_results.append(result)\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        if batch_results:\n",
    "            batch_df = pd.DataFrame(batch_results).set_index(\"original_index\")\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            output_file = os.path.join(output_dir, f\"batch_{start}_{end}_{timestamp}.parquet\")\n",
    "            batch_df.to_parquet(output_file, index=True)\n",
    "            print(f\"Saved batch {start} to {end} with {len(batch_results)} records to {output_file}\")\n",
    "            batch_files.append(output_file)\n",
    "        else:\n",
    "            print(f\"No records processed in batch {start} to {end}.\")\n",
    "    \n",
    "    return batch_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\"sampled_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all parquet files in the output_batches folder to create a complete dataset for modeling\n",
    "import glob\n",
    "parquet_files = glob.glob(os.path.join(\"output_batches\", '*.parquet'))\n",
    "\n",
    "# Read each parquet file into a DataFrame and store them in a list\n",
    "df_list = [pd.read_parquet(file) for file in parquet_files]\n",
    "\n",
    "# Concatenate all the DataFrames and reset the index\n",
    "processed_sample = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Optionally, display the first few rows of the combined DataFrame\n",
    "print(processed_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only save the successfully processed songs to a parquet file for lyric embeddings extraction\n",
    "successfully_processed_sample=processed_sample[processed_sample[\"failed\"]==False]\n",
    "successfully_processed_sample.to_parquet(\"processed_sample.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lyrics Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "genius=lyricsgenius.Genius('YOUR-GENIUSAPI-ACCESS-TOKEN',timeout=10,retries=2) \n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2') #Transformer-based LLM\n",
    "def extract_lyrics_features(title,artist):\n",
    "    \"\"\"\n",
    "    Given a string of lyrics, this function uses a BERT-based model\n",
    "    (via Sentence Transformers) to extract semantic features as an embedding vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lyrics = genius.search_song(title, artist).lyrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving for '{title}' by '{artist}': {e}\")\n",
    "        return np.zeros(768)\n",
    "    \n",
    "    # Using all-mpnet-base-v2 for sentence embeddings\n",
    "    embedding = model.encode(lyrics)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successfully_processed_sample['lyrics_embeddings'] = successfully_processed_sample.apply(lambda row: extract_lyrics_features(row['title'], row['artist']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the embeddings from numpy array to lists to save it in a parquet file\n",
    "successfully_processed_sample['lyrics_embeddings'] = successfully_processed_sample['lyrics_embeddings'].apply(lambda x: np.asarray(x, dtype=np.float64).tolist() if isinstance(x, (list, np.ndarray)) else x)\n",
    "successfully_processed_sample.to_parquet(\"processed_sample.parquet\") #Rewrite the parquet file with the final dataset"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
